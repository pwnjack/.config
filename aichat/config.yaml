# aichat configuration
# https://github.com/sigoden/aichat/blob/main/config.example.yaml

# ---- Model ----
model: gemini:gemini-2.5-flash

# ---- Behavior ----
stream: true
save: true
keybindings: emacs
wrap: auto
wrap_code: false

# ---- Session ----
save_session: true
compress_threshold: 4000
summarize_prompt: "Summarize the discussion briefly in 200 words or less to use as a prompt for future context."
summary_prompt: "This is a summary of the chat history as a recap: "

# ---- Appearance ----
highlight: true
light_theme: false
left_prompt: "{color.green}{model}{color.cyan}>{color.reset} "
right_prompt: "{color.purple}{?consume_tokens {consume_tokens}}{color.reset}"

# ---- Misc ----
save_shell_history: true

# ---- Clients ----
# Switch models with: .model <provider>:<model-name>
# List available: .model
clients:
  # Google Gemini
  # API key loaded from GEMINI_API_KEY environment variable
  - type: gemini
    # Models: gemini-2.5-flash, gemini-2.5-pro, gemini-2.0-flash

# OpenAI / ChatGPT
# Set OPENAI_API_KEY environment variable
# - type: openai
#   # Models: gpt-4o, gpt-4o-mini, gpt-4-turbo, o1, o1-mini

# Anthropic / Claude
# Set ANTHROPIC_API_KEY environment variable
# - type: claude
#   # Models: claude-sonnet-4-20250514, claude-opus-4-20250514, claude-3-5-haiku-20241022

# Groq (fast inference)
# Set GROQ_API_KEY environment variable
# - type: groq
#   # Models: llama-3.3-70b-versatile, mixtral-8x7b-32768

# Mistral
# Set MISTRAL_API_KEY environment variable
# - type: mistral
#   # Models: mistral-large-latest, mistral-small-latest, codestral-latest

# OpenRouter (access to many models)
# Set OPENROUTER_API_KEY environment variable
# - type: openai-compatible
#   name: openrouter
#   api_base: https://openrouter.ai/api/v1
#   # Models: anthropic/claude-3.5-sonnet, openai/gpt-4o, google/gemini-pro-1.5

# Local Ollama
# - type: ollama
#   api_base: http://localhost:11434
#   # Models: llama3, codellama, mistral, phi3
